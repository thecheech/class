{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "Spark is a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing.\n",
    "\n",
    "Spark applications can be written in Python, Java, Scala in R. It integrates well with IPython and the entire Python Stack (e.g. Numpy).\n",
    "\n",
    "The company Databricks is the main contributor to the open-source project. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This graph, when published, created much excitement about Spark (and some controversy):\n",
    "\n",
    "![Spark](http://spark.apache.org/images/logistic-regression.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Spark includes multiple modules:\n",
    "\n",
    "![spark-modules](http://spark.apache.org/images/spark-stack.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A spark cluster contains a master and workers:\n",
    "\n",
    "![spark-master-worker](https://camo.githubusercontent.com/8db49b5f39c2ba95614d1fbe98e905b4694f9999/687474703a2f2f737061726b2e6170616368652e6f72672f646f63732f6c61746573742f696d672f636c75737465722d6f766572766965772e706e67)\n",
    "![executors](http://spark-mooc.github.io/web-assets/images/executors.png)\n",
    "\n",
    "\n",
    "This is a quick introduction. Please refer to the programming guide if you want to go deeper:\n",
    "http://spark.apache.org/docs/latest/programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a sparkcontext (unless created by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Running Spark Version %s\" % (sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparkcontext comes with a default configuration for local computations\n",
    "\n",
    "by modifying SparkConf we can set our cluster configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "\n",
    "conf.set(\"spark.python.profile\", \"true\", \n",
    "         # ...\n",
    "        )\n",
    "\n",
    "# Read more about configurations here: http://spark.apache.org/docs/latest/configuration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "\n",
    "Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is a distributed collection of objects. Each RDD is divided into partitions, which may be computed on different nodes of the cluster.\n",
    "\n",
    "RDDs support two types of operations: \n",
    "\n",
    "1. **Transformations** which create a new dataset from an existing one\n",
    "\n",
    "2. **Actions** which return a value to the driver program after running a computation on the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load a range of 0 .. 10000 (Python)\n",
    "\n",
    "data = range(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parallelize the list, create an RDD\n",
    "\n",
    "dataRDD = sc.parallelize(\n",
    "    data, # python collection that will be distributed\n",
    "    4 # number of slices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can collect an RDD\n",
    "dataRDD.collect()\n",
    "\n",
    "# or take first 5\n",
    "dataRDD.take(5)\n",
    "\n",
    "# (this is an action - it requires data to all be in the same place!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's substract 1 from every element in the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subRDD = dataRDD.map(lambda x: x - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the a list of selected operations:\n",
    "\n",
    "**Transformations** :\n",
    "\n",
    "* map\n",
    "* filter\n",
    "* sample\n",
    "* groupByKey\n",
    "* reduceByKey\n",
    "\n",
    "[cf. doc](http://spark.apache.org/docs/latest/programming-guide.html#transformations)\n",
    "\n",
    "**Actions** :\n",
    "\n",
    "* first()\n",
    "* count()\n",
    "* take()\n",
    "* collect()\n",
    "* reduce()\n",
    "* takeOrdered() : sort according to a lambda (passed as the second variable)\n",
    "* top()\n",
    "\n",
    "[cf. doc](http://spark.apache.org/docs/latest/programming-guide.html#actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete from the collections elements that are multiples of 3 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataRDD.filter(lambda x: x % 2 != 0 and x % 5 != 0).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataRDD.filter(lambda x: x % 2 != 0 and x % 5 != 0) \\\n",
    "    .reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByValue (similar to value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repetitiveRDD = sc.parallelize([1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 3, 3, 4, 5, 4, 6])\n",
    "print(repetitiveRDD.countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map et flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpleRDD = sc.parallelize([2, 3, 4])\n",
    "print(simpleRDD.map(lambda x: range(1, x)).collect())\n",
    "\n",
    "#  one-to-many mapping\n",
    "print(simpleRDD.flatMap(lambda x: range(1, x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's create a new base RDD to work from\n",
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "\n",
    "# Use map\n",
    "singularAndPluralWordsRDDMap = wordsRDD.map(lambda x: (x, x + 's'))\n",
    "# Use flatMap\n",
    "singularAndPluralWordsRDD = wordsRDD.flatMap(lambda x: (x, x + 's'))\n",
    "\n",
    "# View the results\n",
    "print(singularAndPluralWordsRDDMap.collect())\n",
    "print(singularAndPluralWordsRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "\n",
    "![reduceByKey() figure](http://spark-mooc.github.io/web-assets/images/reduce_by.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n",
    "\n",
    "redRDD = sc.parallelize(data, 4)\n",
    "\n",
    "redRDD = redRDD.map(lambda x: (x, 1))\n",
    "    \n",
    "redRDD = redRDD.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redRDD.collect() # we just did sum aggregation by key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = sc.textFile(\"data/titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count elements in collections\n",
    "\n",
    "titanic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic.takeSample(False, 10, 1) # Return a fixed-size sampled subset of this RDD\n",
    "\n",
    "# takeSample(withReplacement, num, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = titanic.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = titanic.filter(lambda x: x != '').filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_row(row):\n",
    "    row = [segs.replace('\"','') for segs in row.split(',')]\n",
    "    return {\n",
    "        'survived': row[1],\n",
    "        'pclass': row[2],\n",
    "        'sex': row[5],\n",
    "        'age': row[6],\n",
    "        'ticket': row[9],\n",
    "        'fare': float(row[10]) if '.' in row[10] else -1,\n",
    "        'embarked': row[12]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_titanic = titanic.map(lambda row: parse_row(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_titanic.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very bulky process ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_titanic.map(lambda person: person['sex']).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the average fare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_titanic.map(lambda x: x['fare']).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fares = parsed_titanic.map(lambda x: x['fare']).filter(lambda x: x != -1)\n",
    "fares = fares.reduce(lambda a, b: a + b) / fares.count()\n",
    "fares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_titanic.filter(lambda row: row['fare'] != -1) \\\n",
    "    .map(lambda row: (row['pclass'], (row['fare'], 1))) \\\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "    .mapValues(lambda u: u[0] / u[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so much fun to write this long low-level operations ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(parsed_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sample(False, 0.1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select(\"pclass\", \"survived\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select(\"pclass\", \"survived\").filter(df[\"survived\"] == 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"sex\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#default format is parquet\n",
    "\n",
    "df.write.save('output/titanic.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = sqlContext.read.parquet('output/titanic.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby(\"pclass\") \\\n",
    "    .agg(func.mean(\"fare\"), func.mean(\"survived\")) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby(\"sex\") \\\n",
    "    .agg(func.sum(\"fare\").alias(\"total\")) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - We can run an sql query on a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT count(*) FROM parquet.`output/titanic.parquet`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - We can registers an existing RDD as a SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "    SELECT pclass, avg(fare)\n",
    "    FROM titanic\n",
    "    GROUP BY pclass\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "referential = sc.parallelize([('S', 'Southampton'), ('C', 'Cherbourg'), ('Q', 'Queenstown')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_df = sqlContext.createDataFrame(referential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_df = ref_df.withColumnRenamed('_1', 'embarked') \\\n",
    "    .withColumnRenamed('_2', 'port_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.join(ref_df, on='embarked', how='left').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.join(ref_df, on='embarked', how='left').filter(df['embarked'] == 'S').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to prepare the data. MLlib models use **LabeledPoint** as input (features and labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by predicting survival (survived=1) using a Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def create_point(row):    \n",
    "    features = [\n",
    "        int(row['pclass']) - 1,\n",
    "        (1 if row['age'] > '18' else 0),\n",
    "        (1 if row['sex'] == 'female' else 0)\n",
    "    ]\n",
    "    return LabeledPoint(1 if row['survived'] == '1' else 0, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_points = parsed_titanic.map(create_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_points.takeSample(False, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's validate out model using the split funcion randomSplit:\n",
    "\n",
    "training_rdd, test_rdd = titanic_points.randomSplit([0.7, 0.3], seed = 0)\n",
    "\n",
    "training_count = training_rdd.count()\n",
    "test_count = test_rdd.count()\n",
    "\n",
    "# size of two new data sets:\n",
    "training_count, test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "lr = LogisticRegressionWithLBFGS.train(training_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_rdd = lr.predict(test_rdd.map(lambda x: x.features))\n",
    "predictions_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first let's inspect test dataset\n",
    "\n",
    "test_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zip allows us to transform two lists into a list of tuples\n",
    "\n",
    "truth_and_predictions_rdd = test_rdd.map(lambda x: x.label).zip(predictions_rdd)\n",
    "truth_and_predictions_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = truth_and_predictions_rdd.filter(lambda v_p: v_p[0] == v_p[1]).count() / float(test_count)\n",
    "print('Accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "dt=DecisionTree.trainClassifier(\n",
    "   training_rdd, \n",
    "   numClasses=2, \n",
    "    \n",
    "   categoricalFeaturesInfo={\n",
    "        # Map from categorical feature index to number of categories. \n",
    "        # Any feature not in this map is treated as continuous.\n",
    "        0: 3,\n",
    "        1: 2,\n",
    "        2: 2\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_rdd = dt.predict(test_rdd.map(lambda x: x.features))\n",
    "predictions_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truth_and_predictions_rdd = test_rdd.map(lambda x: x.label).zip(predictions_rdd)\n",
    "truth_and_predictions_rdd.take(5)\n",
    "\n",
    "accuracy = truth_and_predictions_rdd.filter(lambda v_p: v_p[0] == v_p[1]).count() / float(test_count)\n",
    "print('Accuracy =', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
